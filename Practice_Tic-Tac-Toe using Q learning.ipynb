{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPgA44POkP1eoDpQE5+O5ag"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# R025 - Rohan Jain\n","# R054 - Archisha Sinha\n","\n","## Domain: Reinforcement Learning\n","## Topic: Tic-Tac-Toe using Q-learning"],"metadata":{"id":"0xHSwA7F-bM3"}},{"cell_type":"markdown","source":["## **Q-learning Algorithm:**\n","\n","Q-learning is a model-free reinforcement learning algorithm used to find the optimal action-selection policy using the Bellman equation. The agent learns the Q-value (action-value) function to estimate the reward for each state-action pair and updates its knowledge based on the environment's feedback.\n"],"metadata":{"id":"7hfwpJc2_Qpe"}},{"cell_type":"markdown","source":["**Steps of Q-learning:**\n","\n","- Initialize the Q-table, where each state is mapped to an array representing action values.\n","- Choose an action using an epsilon-greedy policy (where a random action is taken with probability epsilon, and the best-known action is taken otherwise).\n","- Execute the action in the environment, observe the next state, and get the reward.\n","- Update the Q-value for the current state-action pair using the Bellman equation:\n","\n","\n"],"metadata":{"id":"QW99qrvM_h9_"}},{"cell_type":"markdown","source":["![image.png](data:image/png;base64,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)"],"metadata":{"id":"lG0gFjU-_sQc"}},{"cell_type":"markdown","source":["- Repeat the process for multiple episodes until the policy converges."],"metadata":{"id":"IaF4f_YG_wMp"}},{"cell_type":"markdown","source":["**Problem Statement**\n","\n","The task is to create a Tic-Tac-Toe game where an RL agent learns to play by training against itself. The board is a 3x3 grid, and the two players alternate marking X and O. The agent receives a reward of +1 for winning, 0.5 for a draw, and 0 for any other action.\n"],"metadata":{"id":"FQWV7j17_1Lb"}},{"cell_type":"code","source":["import numpy as np\n","import random"],"metadata":{"id":"o9JeydjT_fsE","executionInfo":{"status":"ok","timestamp":1727516474336,"user_tz":-330,"elapsed":617,"user":{"displayName":"Archisha Sinha","userId":"04837774794656308874"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"mILAlJgt-RzT","executionInfo":{"status":"ok","timestamp":1727516768391,"user_tz":-330,"elapsed":439,"user":{"displayName":"Archisha Sinha","userId":"04837774794656308874"}}},"outputs":[],"source":["# Initialize the environment\n","board = [' ' for _ in range(9)]  # Tic-Tac-Toe board (3x3)\n","q_table = {}  # Initialize Q-table\n","epsilon = 0.1  # Exploration rate\n","alpha = 0.1  # Learning rate\n","gamma = 0.9  # Discount factor\n","player1 = 'X'\n","player2 = 'O'\n","episodes = 10000"]},{"cell_type":"code","source":["# Functions to check game status\n","def check_winner(board, player):\n","    win_conditions = [(0, 1, 2), (3, 4, 5), (6, 7, 8),  # rows\n","                      (0, 3, 6), (1, 4, 7), (2, 5, 8),  # columns\n","                      (0, 4, 8), (2, 4, 6)]  # diagonals\n","    for condition in win_conditions:\n","        if board[condition[0]] == board[condition[1]] == board[condition[2]] == player:\n","            return True\n","    return False"],"metadata":{"id":"6Qey8umgBfQl","executionInfo":{"status":"ok","timestamp":1727516775181,"user_tz":-330,"elapsed":426,"user":{"displayName":"Archisha Sinha","userId":"04837774794656308874"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def is_draw(board):\n","    return ' ' not in board"],"metadata":{"id":"UsqNpW3TBg62","executionInfo":{"status":"ok","timestamp":1727516791876,"user_tz":-330,"elapsed":465,"user":{"displayName":"Archisha Sinha","userId":"04837774794656308874"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def available_actions(board):\n","    return [i for i in range(9) if board[i] == ' ']"],"metadata":{"id":"oHHC1TZUBk-_","executionInfo":{"status":"ok","timestamp":1727516798540,"user_tz":-330,"elapsed":480,"user":{"displayName":"Archisha Sinha","userId":"04837774794656308874"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Convert the board to a tuple (state representation)\n","def get_state(board):\n","    return tuple(board)\n"],"metadata":{"id":"Hq4i5kitBmmq","executionInfo":{"status":"ok","timestamp":1727516805101,"user_tz":-330,"elapsed":455,"user":{"displayName":"Archisha Sinha","userId":"04837774794656308874"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Update Q-table using the Bellman equation\n","def update_q_table(state, action, reward, next_state, done):\n","    if state not in q_table:\n","        q_table[state] = np.zeros(9)  # Initialize Q-values for all actions\n","\n","    if next_state not in q_table:\n","        q_table[next_state] = np.zeros(9)  # Initialize for next state if unseen\n","\n","    if done:\n","        q_table[state][action] = reward  # No next state if done\n","    else:\n","        q_table[state][action] = q_table[state][action] + alpha * (reward + gamma * np.max(q_table[next_state]) - q_table[state][action])"],"metadata":{"id":"T0uGSsHCBoN2","executionInfo":{"status":"ok","timestamp":1727516816168,"user_tz":-330,"elapsed":468,"user":{"displayName":"Archisha Sinha","userId":"04837774794656308874"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Main loop to run Q-learning\n","for episode in range(episodes):\n","    board = [' ' for _ in range(9)]  # Reset the board\n","    done = False\n","    player_turn = player1  # Player 1 starts\n","    state = get_state(board)\n","\n","    while not done:\n","        # Choose action\n","        if random.uniform(0, 1) < epsilon:  # Exploration\n","            action = random.choice(available_actions(board))\n","        else:  # Exploitation\n","            q_values = q_table.get(state, np.zeros(9))\n","            action = np.argmax(q_values)\n","\n","        # Make the move\n","        board[action] = player_turn\n","        next_state = get_state(board)\n","\n","        # Check if the game is over\n","        if check_winner(board, player_turn):\n","            reward = 1  # Win reward\n","            update_q_table(state, action, reward, next_state, done=True)\n","            done = True\n","        elif is_draw(board):\n","            reward = 0.5  # Draw reward\n","            update_q_table(state, action, reward, next_state, done=True)\n","            done = True\n","        else:\n","            reward = 0  # No reward yet\n","            update_q_table(state, action, reward, next_state, done=False)\n","            state = next_state\n","\n","        # Switch player\n","        player_turn = player2 if player_turn == player1 else player1"],"metadata":{"id":"oRSHlDjoBq6Z","executionInfo":{"status":"ok","timestamp":1727516831904,"user_tz":-330,"elapsed":2121,"user":{"displayName":"Archisha Sinha","userId":"04837774794656308874"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Testing the learned policy\n","board = [' ' for _ in range(9)]\n","state = get_state(board)\n","done = False\n","\n","while not done:\n","    q_values = q_table.get(state, np.zeros(9))\n","    action = np.argmax(q_values)  # Pick the best learned action\n","    board[action] = player1  # Play as 'X'\n","    print(np.array(board).reshape(3, 3), \"\\n\")  # Print board\n","\n","    if check_winner(board, player1):\n","        print(\"Player X wins!\")\n","        break\n","    elif is_draw(board):\n","        print(\"It's a draw!\")\n","        break\n","\n","    # Simulate a random opponent move\n","    opponent_action = random.choice(available_actions(board))\n","    board[opponent_action] = player2\n","    state = get_state(board)\n","\n","    if check_winner(board, player2):\n","        print(np.array(board).reshape(3, 3))\n","        print(\"Player O wins!\")\n","        break\n","    elif is_draw(board):\n","        print(\"It's a draw!\")\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rlzavbOhBuWl","executionInfo":{"status":"ok","timestamp":1727516843190,"user_tz":-330,"elapsed":430,"user":{"displayName":"Archisha Sinha","userId":"04837774794656308874"}},"outputId":"b5cf66d8-83d7-46ca-f614-bfdaf4c595c6"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["[[' ' ' ' ' ']\n"," ['X' ' ' ' ']\n"," [' ' ' ' ' ']] \n","\n","[['X' ' ' ' ']\n"," ['X' ' ' ' ']\n"," [' ' ' ' 'O']] \n","\n","[['X' ' ' ' ']\n"," ['X' 'O' ' ']\n"," [' ' ' ' 'O']] \n","\n","[['X' 'O' ' ']\n"," ['X' 'O' ' ']\n"," [' ' ' ' 'O']] \n","\n","[['X' 'O' ' ']\n"," ['X' 'O' ' ']\n"," ['O' ' ' 'O']] \n","\n","[['X' 'O' ' ']\n"," ['X' 'O' 'O']\n"," ['O' ' ' 'O']] \n","\n","[['X' 'O' ' ']\n"," ['X' 'O' 'O']\n"," ['O' 'O' 'O']]\n","Player O wins!\n"]}]},{"cell_type":"markdown","source":["**Conclusion:**\n","- The Q-learning agent successfully learned to play Tic-Tac-Toe by training against itself, developing strategies to win or force draws after multiple episodes.\n","- The epsilon-greedy policy allowed the agent to explore different strategies initially, while later focusing on exploiting its learned Q-values for optimal play.\n","- The RL agent’s performance showed that with proper tuning of hyperparameters like alpha and gamma, the agent could efficiently learn the game mechanics and improve over time.\n","- This experiment demonstrated the effectiveness of Q-learning in enabling an agent to make intelligent decisions in a simple game environment without the need for predefined strategies."],"metadata":{"id":"jPvO63nTCCf2"}},{"cell_type":"code","source":[],"metadata":{"id":"nWONCeRsBxfx"},"execution_count":null,"outputs":[]}]}